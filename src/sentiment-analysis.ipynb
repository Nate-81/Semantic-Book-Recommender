{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2015c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a2760f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(\"../data/books_with_categories.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d714cc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/Users/nathans./Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'anger', 'score': 0.004419783595949411},\n",
       "  {'label': 'disgust', 'score': 0.001611992483958602},\n",
       "  {'label': 'fear', 'score': 0.00041385198710486293},\n",
       "  {'label': 'joy', 'score': 0.9771687984466553},\n",
       "  {'label': 'neutral', 'score': 0.00576459476724267},\n",
       "  {'label': 'sadness', 'score': 0.002092391485348344},\n",
       "  {'label': 'surprise', 'score': 0.00852868054062128}]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", \n",
    "                      return_all_scores=True)\n",
    "\n",
    "classifier(\"I love this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8ce0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier(books['description'][0].split(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0bf3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = [i[\"label\"] for i in predictions[0]]\n",
    "isbn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "520cf0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "754ade50",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_scores = {label: [] for label in emotion_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c38987c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_emotion_scores(prediction):\n",
    "    per_emotion_scores = {label: [] for label in emotion_labels}\n",
    "    for prediction in predictions:\n",
    "        sorted_predictions = sorted(prediction, key=lambda x : x[\"label\"])\n",
    "        for index, label in enumerate(emotion_labels):\n",
    "            per_emotion_scores[label].append(sorted_predictions[index][\"score\"])\n",
    "    return {label : np.max(scores) for label, scores in per_emotion_scores.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "52ce7277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/5088 [00:00<17:18,  4.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m isbn\u001b[38;5;241m.\u001b[39mappend(books[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misbn13\u001b[39m\u001b[38;5;124m\"\u001b[39m][i])\n\u001b[1;32m      3\u001b[0m sentences \u001b[38;5;241m=\u001b[39m books[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m max_scores \u001b[38;5;241m=\u001b[39m calculate_max_emotion_scores(predictions)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m emotion_labels:\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:163\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 163\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1445\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1442\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1443\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1444\u001b[0m     )\n\u001b[0;32m-> 1445\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[1;32m   1446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1371\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1370\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1371\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:194\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    193\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1191\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;124;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1191\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:794\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    792\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 794\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    803\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:87\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;66;03m# Create the position ids from the input token ids. Any padded tokens remain padded.\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_position_ids_from_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_position_ids_from_inputs_embeds(inputs_embeds)\n",
      "File \u001b[0;32m~/Documents/LLM_Project/Project/.venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1553\u001b[0m, in \u001b[0;36mcreate_position_ids_from_input_ids\u001b[0;34m(input_ids, padding_idx, past_key_values_length)\u001b[0m\n\u001b[1;32m   1551\u001b[0m mask \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mne(padding_idx)\u001b[38;5;241m.\u001b[39mint()\n\u001b[1;32m   1552\u001b[0m incremental_indices \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mcumsum(mask, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(mask) \u001b[38;5;241m+\u001b[39m past_key_values_length) \u001b[38;5;241m*\u001b[39m mask\n\u001b[0;32m-> 1553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mincremental_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m padding_idx\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(books))):\n",
    "    isbn.append(books[\"isbn13\"][i])\n",
    "    sentences = books[\"description\"][i].split(\".\")\n",
    "    predictions = classifier(sentences)\n",
    "    max_scores = calculate_max_emotion_scores(predictions)\n",
    "    for label in emotion_labels:\n",
    "        emotion_scores[label].append(max_scores[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb93fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_df = pd.DataFrame(emotion_scores)\n",
    "emotions_df['isbn13'] = isbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e8928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>neutral</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>isbn13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.273590</td>\n",
       "      <td>0.928169</td>\n",
       "      <td>0.932798</td>\n",
       "      <td>0.646216</td>\n",
       "      <td>0.967158</td>\n",
       "      <td>0.729602</td>\n",
       "      <td>9780002005883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.612619</td>\n",
       "      <td>0.348286</td>\n",
       "      <td>0.942528</td>\n",
       "      <td>0.704422</td>\n",
       "      <td>0.887940</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.252544</td>\n",
       "      <td>9780002261982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.104007</td>\n",
       "      <td>0.972321</td>\n",
       "      <td>0.767237</td>\n",
       "      <td>0.549477</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.078765</td>\n",
       "      <td>9780006178736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.351483</td>\n",
       "      <td>0.150722</td>\n",
       "      <td>0.360707</td>\n",
       "      <td>0.251881</td>\n",
       "      <td>0.732687</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.078765</td>\n",
       "      <td>9780006280897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.081412</td>\n",
       "      <td>0.184495</td>\n",
       "      <td>0.095043</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.884389</td>\n",
       "      <td>0.475881</td>\n",
       "      <td>0.078765</td>\n",
       "      <td>9780006280934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>0.148208</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>0.919165</td>\n",
       "      <td>0.255170</td>\n",
       "      <td>0.853723</td>\n",
       "      <td>0.980877</td>\n",
       "      <td>0.030656</td>\n",
       "      <td>9788172235222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5084</th>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.114383</td>\n",
       "      <td>0.051363</td>\n",
       "      <td>0.400263</td>\n",
       "      <td>0.883198</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.227765</td>\n",
       "      <td>9788173031014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5085</th>\n",
       "      <td>0.009997</td>\n",
       "      <td>0.009929</td>\n",
       "      <td>0.339217</td>\n",
       "      <td>0.947779</td>\n",
       "      <td>0.375755</td>\n",
       "      <td>0.066685</td>\n",
       "      <td>0.057625</td>\n",
       "      <td>9788179921623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.104007</td>\n",
       "      <td>0.459270</td>\n",
       "      <td>0.759455</td>\n",
       "      <td>0.951104</td>\n",
       "      <td>0.368110</td>\n",
       "      <td>0.078765</td>\n",
       "      <td>9788185300535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.104007</td>\n",
       "      <td>0.051363</td>\n",
       "      <td>0.958549</td>\n",
       "      <td>0.915193</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.078765</td>\n",
       "      <td>9789027712059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5088 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         anger   disgust      fear       joy   neutral   sadness  surprise  \\\n",
       "0     0.064134  0.273590  0.928169  0.932798  0.646216  0.967158  0.729602   \n",
       "1     0.612619  0.348286  0.942528  0.704422  0.887940  0.111690  0.252544   \n",
       "2     0.064134  0.104007  0.972321  0.767237  0.549477  0.111690  0.078765   \n",
       "3     0.351483  0.150722  0.360707  0.251881  0.732687  0.111690  0.078765   \n",
       "4     0.081412  0.184495  0.095043  0.040564  0.884389  0.475881  0.078765   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5083  0.148208  0.030643  0.919165  0.255170  0.853723  0.980877  0.030656   \n",
       "5084  0.064134  0.114383  0.051363  0.400263  0.883198  0.111690  0.227765   \n",
       "5085  0.009997  0.009929  0.339217  0.947779  0.375755  0.066685  0.057625   \n",
       "5086  0.064134  0.104007  0.459270  0.759455  0.951104  0.368110  0.078765   \n",
       "5087  0.064134  0.104007  0.051363  0.958549  0.915193  0.111690  0.078765   \n",
       "\n",
       "             isbn13  \n",
       "0     9780002005883  \n",
       "1     9780002261982  \n",
       "2     9780006178736  \n",
       "3     9780006280897  \n",
       "4     9780006280934  \n",
       "...             ...  \n",
       "5083  9788172235222  \n",
       "5084  9788173031014  \n",
       "5085  9788179921623  \n",
       "5086  9788185300535  \n",
       "5087  9789027712059  \n",
       "\n",
       "[5088 rows x 8 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.merge(books, emotions_df, on=\"isbn13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95fa21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>neutral</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>isbn13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5088.000000</td>\n",
       "      <td>5088.000000</td>\n",
       "      <td>5088.000000</td>\n",
       "      <td>5088.000000</td>\n",
       "      <td>5088.000000</td>\n",
       "      <td>5088.000000</td>\n",
       "      <td>5088.000000</td>\n",
       "      <td>5.088000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.166382</td>\n",
       "      <td>0.202044</td>\n",
       "      <td>0.311614</td>\n",
       "      <td>0.283588</td>\n",
       "      <td>0.762585</td>\n",
       "      <td>0.225002</td>\n",
       "      <td>0.176143</td>\n",
       "      <td>9.780667e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.219698</td>\n",
       "      <td>0.213456</td>\n",
       "      <td>0.343393</td>\n",
       "      <td>0.319300</td>\n",
       "      <td>0.202771</td>\n",
       "      <td>0.248600</td>\n",
       "      <td>0.190160</td>\n",
       "      <td>5.977396e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>9.780002e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.104007</td>\n",
       "      <td>0.051363</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.549477</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.078765</td>\n",
       "      <td>9.780313e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.104007</td>\n",
       "      <td>0.097962</td>\n",
       "      <td>0.091727</td>\n",
       "      <td>0.840248</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.078765</td>\n",
       "      <td>9.780521e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.141981</td>\n",
       "      <td>0.190982</td>\n",
       "      <td>0.592009</td>\n",
       "      <td>0.511456</td>\n",
       "      <td>0.936846</td>\n",
       "      <td>0.182212</td>\n",
       "      <td>0.204457</td>\n",
       "      <td>9.780807e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.989582</td>\n",
       "      <td>0.989417</td>\n",
       "      <td>0.995326</td>\n",
       "      <td>0.992068</td>\n",
       "      <td>0.974344</td>\n",
       "      <td>0.989361</td>\n",
       "      <td>0.983455</td>\n",
       "      <td>9.789028e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             anger      disgust         fear          joy      neutral  \\\n",
       "count  5088.000000  5088.000000  5088.000000  5088.000000  5088.000000   \n",
       "mean      0.166382     0.202044     0.311614     0.283588     0.762585   \n",
       "std       0.219698     0.213456     0.343393     0.319300     0.202771   \n",
       "min       0.000769     0.000821     0.000442     0.000556     0.000981   \n",
       "25%       0.064134     0.104007     0.051363     0.040564     0.549477   \n",
       "50%       0.064134     0.104007     0.097962     0.091727     0.840248   \n",
       "75%       0.141981     0.190982     0.592009     0.511456     0.936846   \n",
       "max       0.989582     0.989417     0.995326     0.992068     0.974344   \n",
       "\n",
       "           sadness     surprise        isbn13  \n",
       "count  5088.000000  5088.000000  5.088000e+03  \n",
       "mean      0.225002     0.176143  9.780667e+12  \n",
       "std       0.248600     0.190160  5.977396e+08  \n",
       "min       0.001251     0.000779  9.780002e+12  \n",
       "25%       0.111690     0.078765  9.780313e+12  \n",
       "50%       0.111690     0.078765  9.780521e+12  \n",
       "75%       0.182212     0.204457  9.780807e+12  \n",
       "max       0.989361     0.983455  9.789028e+12  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "57cf4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "books.to_csv(\"../data/books_with_emotions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "67aaf6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn13</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>description</th>\n",
       "      <th>published_year</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>title_and_subtitle</th>\n",
       "      <th>tagged_description</th>\n",
       "      <th>simple_categories</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>neutral</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9780002005883</td>\n",
       "      <td>Marilynne Robinson</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>A NOVEL THAT READERS and critics have been eag...</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>247.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>Gilead</td>\n",
       "      <td>9780002005883 A NOVEL THAT READERS and critics...</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.273590</td>\n",
       "      <td>0.928169</td>\n",
       "      <td>0.932798</td>\n",
       "      <td>0.646216</td>\n",
       "      <td>0.967158</td>\n",
       "      <td>0.729602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9780002261982</td>\n",
       "      <td>Charles Osborne;Agatha Christie</td>\n",
       "      <td>Detective and mystery stories</td>\n",
       "      <td>A new 'Christie for Christmas' -- a full-lengt...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3.83</td>\n",
       "      <td>241.0</td>\n",
       "      <td>5164.0</td>\n",
       "      <td>Spider's Web: A Novel</td>\n",
       "      <td>9780002261982 A new 'Christie for Christmas' -...</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>0.612619</td>\n",
       "      <td>0.348286</td>\n",
       "      <td>0.942528</td>\n",
       "      <td>0.704422</td>\n",
       "      <td>0.887940</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.252544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9780006178736</td>\n",
       "      <td>Sidney Sheldon</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>A memorable, mesmerizing heroine Jennifer -- b...</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>3.93</td>\n",
       "      <td>512.0</td>\n",
       "      <td>29532.0</td>\n",
       "      <td>Rage of angels</td>\n",
       "      <td>9780006178736 A memorable, mesmerizing heroine...</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.104007</td>\n",
       "      <td>0.972321</td>\n",
       "      <td>0.767237</td>\n",
       "      <td>0.549477</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.078765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9780006280897</td>\n",
       "      <td>Clive Staples Lewis</td>\n",
       "      <td>Christian life</td>\n",
       "      <td>Lewis' work on the nature of love divides love...</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>4.15</td>\n",
       "      <td>170.0</td>\n",
       "      <td>33684.0</td>\n",
       "      <td>The Four Loves</td>\n",
       "      <td>9780006280897 Lewis' work on the nature of lov...</td>\n",
       "      <td>Philosophy</td>\n",
       "      <td>0.351483</td>\n",
       "      <td>0.150722</td>\n",
       "      <td>0.360707</td>\n",
       "      <td>0.251881</td>\n",
       "      <td>0.732687</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.078765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9780006280934</td>\n",
       "      <td>Clive Staples Lewis</td>\n",
       "      <td>Christian life</td>\n",
       "      <td>\"In The Problem of Pain, C.S. Lewis, one of th...</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>4.09</td>\n",
       "      <td>176.0</td>\n",
       "      <td>37569.0</td>\n",
       "      <td>The Problem of Pain</td>\n",
       "      <td>9780006280934 \"In The Problem of Pain, C.S. Le...</td>\n",
       "      <td>Religion</td>\n",
       "      <td>0.081412</td>\n",
       "      <td>0.184495</td>\n",
       "      <td>0.095043</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.884389</td>\n",
       "      <td>0.475881</td>\n",
       "      <td>0.078765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          isbn13                          authors  \\\n",
       "0  9780002005883               Marilynne Robinson   \n",
       "1  9780002261982  Charles Osborne;Agatha Christie   \n",
       "2  9780006178736                   Sidney Sheldon   \n",
       "3  9780006280897              Clive Staples Lewis   \n",
       "4  9780006280934              Clive Staples Lewis   \n",
       "\n",
       "                      categories  \\\n",
       "0                        Fiction   \n",
       "1  Detective and mystery stories   \n",
       "2                        Fiction   \n",
       "3                 Christian life   \n",
       "4                 Christian life   \n",
       "\n",
       "                                         description  published_year  \\\n",
       "0  A NOVEL THAT READERS and critics have been eag...          2004.0   \n",
       "1  A new 'Christie for Christmas' -- a full-lengt...          2000.0   \n",
       "2  A memorable, mesmerizing heroine Jennifer -- b...          1993.0   \n",
       "3  Lewis' work on the nature of love divides love...          2002.0   \n",
       "4  \"In The Problem of Pain, C.S. Lewis, one of th...          2002.0   \n",
       "\n",
       "   average_rating  num_pages  ratings_count     title_and_subtitle  \\\n",
       "0            3.85      247.0          361.0                 Gilead   \n",
       "1            3.83      241.0         5164.0  Spider's Web: A Novel   \n",
       "2            3.93      512.0        29532.0         Rage of angels   \n",
       "3            4.15      170.0        33684.0         The Four Loves   \n",
       "4            4.09      176.0        37569.0    The Problem of Pain   \n",
       "\n",
       "                                  tagged_description simple_categories  \\\n",
       "0  9780002005883 A NOVEL THAT READERS and critics...           Fiction   \n",
       "1  9780002261982 A new 'Christie for Christmas' -...           Fiction   \n",
       "2  9780006178736 A memorable, mesmerizing heroine...           Fiction   \n",
       "3  9780006280897 Lewis' work on the nature of lov...        Philosophy   \n",
       "4  9780006280934 \"In The Problem of Pain, C.S. Le...          Religion   \n",
       "\n",
       "      anger   disgust      fear       joy   neutral   sadness  surprise  \n",
       "0  0.064134  0.273590  0.928169  0.932798  0.646216  0.967158  0.729602  \n",
       "1  0.612619  0.348286  0.942528  0.704422  0.887940  0.111690  0.252544  \n",
       "2  0.064134  0.104007  0.972321  0.767237  0.549477  0.111690  0.078765  \n",
       "3  0.351483  0.150722  0.360707  0.251881  0.732687  0.111690  0.078765  \n",
       "4  0.081412  0.184495  0.095043  0.040564  0.884389  0.475881  0.078765  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03347d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97e6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "old_books = pd.read_csv(\"../data/books.csv\")\n",
    "new_books = pd.read_csv(\"../data/books_with_emotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ddc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_books2 = old_books[[\"isbn13\", \"thumbnail\"]]\n",
    "new_books = pd.merge(new_books, old_books2, on=\"isbn13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6997701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_books.to_csv(\"../data/books_with_emotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5da24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
